{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stage 1: Define Information Need and Evaluate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stage 2: Building minimum pipeline\n",
    "\n",
    "### Approche: \n",
    "\n",
    "According to the results of stage 1, we first extracted brand, color, and material information from the product attributes file, and merged them as well as the product description into training data file. \n",
    "\n",
    "Later on we splited text data in the training data into arrays of words under each column, and used Word2Vec as the first transformer to distribute each data set into different and continuous dimensions. \n",
    "\n",
    "Next, we create three features dirived from cosine similarities between Search Term and Product Title, Product Description or Product Attributes respectively. \n",
    "\n",
    "At last, we assembled the three features and put them into Random Forest Regression estimator to build our first experimental pipeline which is later used on testing data, to predict the relevacy score of given search term and product in the testing data file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, DoubleType\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, functions\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql import Row, functions\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mergefunction to use only brand, color and material in product_attributes\n",
    "def mergeFunction(attr):\n",
    "    names = attributes[\"name\"]\n",
    "    values = attributes[\"value\"]\n",
    "    result = []\n",
    "    for name, value in zip(names, values):\n",
    "        if \"Brand\".lower() or \"Color\".lower() or \"Material\".lower() in name.lower():\n",
    "            result.append(value)\n",
    "    return \" \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to find cosine similarity between vectors\n",
    "def cosineSimilarity(v1,v2):\n",
    "    sumOfXs = 0\n",
    "    sumOfYs = 0\n",
    "    sumOfXYs = 0\n",
    "    for i in range(len(v1)):\n",
    "        sumOfXs += v1[i] * v1[i]\n",
    "        sumOfYs +=  v2[i] * v2[i]\n",
    "        sumOfXYs += v2[i] * v1[i]\n",
    "        \n",
    "    return float(sumOfXYs / math.sqrt(sumOfXs * sumOfYs))\n",
    "\n",
    "idfUDF=udf(cosineSimilarity, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all files\n",
    "test_data = pd.read_csv(\"/home/jiawenz1_c4gcp/test.csv\", encoding = 'ISO-8859-1').head(100)\n",
    "train_data = pd.read_csv('/home/jiawenz1_c4gcp/train.csv', encoding = 'ISO-8859-1').head(100)\n",
    "product_description = pd.read_csv('/home/jiawenz1_c4gcp/product_descriptions.csv', encoding = 'ISO-8859-1').head(100)\n",
    "attributes = pd.read_csv('/home/jiawenz1_c4gcp/attributes.csv', encoding = 'ISO-8859-1').head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Processing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# merge train dataframe and description dataframe\n",
    "train_and_description = pd.merge(train_data, product_description, how=\"left\", on=\"product_uid\")\n",
    "attributes.dropna(how=\"all\", inplace=True)\n",
    "attributes[\"product_uid\"] = attributes[\"product_uid\"].astype(int)\n",
    "attributes[\"value\"] = attributes[\"value\"].astype(str)\n",
    "product_attributes = attributes.groupby(\"product_uid\").apply(mergeFunction)\n",
    "product_attributes = product_attributes.reset_index(name=\"product_attributes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+------------------+---------+--------------------+--------------------+\n",
      "| id|product_uid|       product_title|       search_term|relevance| product_description|  product_attributes|\n",
      "+---+-----------+--------------------+------------------+---------+--------------------+--------------------+\n",
      "|  2|     100001|Simpson Strong-Ti...|     angle bracket|      3.0|Not only do angle...|Versatile connect...|\n",
      "|  3|     100001|Simpson Strong-Ti...|         l bracket|      2.5|Not only do angle...|Versatile connect...|\n",
      "|  9|     100002|BEHR Premium Text...|         deck over|      3.0|BEHR Premium Text...|Versatile connect...|\n",
      "| 16|     100005|Delta Vero 1-Hand...|  rain shower head|     2.33|Update your bathr...|                 NaN|\n",
      "| 17|     100005|Delta Vero 1-Hand...|shower only faucet|     2.67|Update your bathr...|                 NaN|\n",
      "+---+-----------+--------------------+------------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# merge train_description dataframe and attribute dataframe\n",
    "train_description_attributes = pd.merge(train_and_description, product_attributes, how=\"left\", on=\"product_uid\")\n",
    "fields = [StructField(\"id\", StringType(), True), StructField(\"product_uid\", StringType(), True), StructField(\"product_title\", StringType(), True), StructField(\"search_term\", StringType(), True)\n",
    "         , StructField(\"relevance\", StringType(), True), StructField(\"product_description\", StringType(), True), StructField(\"product_attributes\", StringType(), True)]\n",
    "mergeSchema = StructType(fields)\n",
    "# convert pandas dataframe to spark dataframe\n",
    "training = sqlContext.createDataFrame(train_description_attributes, mergeSchema)\n",
    "training.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "| id|product_uid|       product_title|         search_term|relevance| product_description|  product_attributes|\n",
      "+---+-----------+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "|  2|     100001|[Simpson, Strong-...|    [angle, bracket]|      3.0|[Not, only, do, a...|[Versatile, conne...|\n",
      "|  3|     100001|[Simpson, Strong-...|        [l, bracket]|      2.5|[Not, only, do, a...|[Versatile, conne...|\n",
      "|  9|     100002|[BEHR, Premium, T...|        [deck, over]|      3.0|[BEHR, Premium, T...|[Versatile, conne...|\n",
      "| 16|     100005|[Delta, Vero, 1-H...|[rain, shower, head]|     2.33|[Update, your, ba...|               [NaN]|\n",
      "| 17|     100005|[Delta, Vero, 1-H...|[shower, only, fa...|     2.67|[Update, your, ba...|               [NaN]|\n",
      "+---+-----------+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# # Processing Dataframe\n",
    "split_col = functions.split(training[3], \" \")\n",
    "training = training.withColumn('search_term', split_col)\n",
    "split_col = functions.split(training[2], \" \")\n",
    "training = training.withColumn('product_title', split_col)\n",
    "split_col = functions.split(training[5], \" \")\n",
    "training = training.withColumn('product_description', split_col)\n",
    "split_col = functions.split(training[6], \" \")\n",
    "training = training.withColumn('product_attributes', split_col)\n",
    "to_double = training[4].cast(DoubleType())\n",
    "training = training.withColumn('relevance', to_double)\n",
    "training.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+--------------------+---------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+\n",
      "| id|product_uid|       product_title|         search_term|relevance| product_description|  product_attributes|search_term_array_features|product_title_array_features|product_description_array_features|product_attributes_array_features|\n",
      "+---+-----------+--------------------+--------------------+---------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+\n",
      "|  2|     100001|[Simpson, Strong-...|    [angle, bracket]|      3.0|[Not, only, do, a...|[Versatile, conne...|      [-0.0027407575398...|        [-0.0210386053659...|              [-0.0264213297573...|             [-0.0068195634406...|\n",
      "|  3|     100001|[Simpson, Strong-...|        [l, bracket]|      2.5|[Not, only, do, a...|[Versatile, conne...|      [0.03617109172046...|        [-0.0210386053659...|              [-0.0264213297573...|             [-0.0068195634406...|\n",
      "|  9|     100002|[BEHR, Premium, T...|        [deck, over]|      3.0|[BEHR, Premium, T...|[Versatile, conne...|      [-0.0339262848719...|        [-0.0067692473530...|              [-0.0041555158975...|             [-0.0068195634406...|\n",
      "| 16|     100005|[Delta, Vero, 1-H...|[rain, shower, head]|     2.33|[Update, your, ba...|               [NaN]|      [-0.0419641186793...|        [0.00966486958965...|              [-0.0288202002993...|             [0.04687127470970...|\n",
      "| 17|     100005|[Delta, Vero, 1-H...|[shower, only, fa...|     2.67|[Update, your, ba...|               [NaN]|      [-0.0255495711850...|        [0.00966486958965...|              [-0.0288202002993...|             [0.04687127470970...|\n",
      "+---+-----------+--------------------+--------------------+---------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use word2Vec to do feature extraction\n",
    "word2Vec = Word2Vec(vectorSize=10, minCount=0, inputCol=\"search_term\", outputCol=\"search_term_array_features\")\n",
    "model = word2Vec.fit(training)\n",
    "training = model.transform(training)\n",
    "word2Vec = Word2Vec(vectorSize=10, minCount=0, inputCol=\"product_title\", outputCol=\"product_title_array_features\")\n",
    "model = word2Vec.fit(training)\n",
    "training = model.transform(training)\n",
    "word2Vec = Word2Vec(vectorSize=10, minCount=0, inputCol=\"product_description\", outputCol=\"product_description_array_features\")\n",
    "model = word2Vec.fit(training)\n",
    "training = model.transform(training)\n",
    "word2Vec = Word2Vec(vectorSize=10, minCount=0, inputCol=\"product_attributes\", outputCol=\"product_attributes_array_features\")\n",
    "model = word2Vec.fit(training)\n",
    "training = model.transform(training)\n",
    "training.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+--------------------+---------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|product_uid|       product_title|         search_term|relevance| product_description|  product_attributes|search_term_array_features|product_title_array_features|product_description_array_features|product_attributes_array_features|      searchAndTitle|searchAndDescription| searchAndAttributes|            features|\n",
      "+---+-----------+--------------------+--------------------+---------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  2|     100001|[Simpson, Strong-...|    [angle, bracket]|      3.0|[Not, only, do, a...|[Versatile, conne...|      [-0.0027407575398...|        [-0.0210386053659...|              [-0.0264213297573...|             [-0.0068195634406...|-0.26561655852971866| 0.11844973328385718|-0.00769678666704...|[-0.2656165585297...|\n",
      "|  3|     100001|[Simpson, Strong-...|        [l, bracket]|      2.5|[Not, only, do, a...|[Versatile, conne...|      [0.03617109172046...|        [-0.0210386053659...|              [-0.0264213297573...|             [-0.0068195634406...| -0.5286525620027563| 0.11503180766077892|    0.10675689938397|[-0.5286525620027...|\n",
      "|  9|     100002|[BEHR, Premium, T...|        [deck, over]|      3.0|[BEHR, Premium, T...|[Versatile, conne...|      [-0.0339262848719...|        [-0.0067692473530...|              [-0.0041555158975...|             [-0.0068195634406...| 0.06326357543589334| -0.6339500869278497|  0.4345611141182554|[0.06326357543589...|\n",
      "| 16|     100005|[Delta, Vero, 1-H...|[rain, shower, head]|     2.33|[Update, your, ba...|               [NaN]|      [-0.0419641186793...|        [0.00966486958965...|              [-0.0288202002993...|             [0.04687127470970...|-0.38332207704016286|   0.096823490542864| -0.6965776092464044|[-0.3833220770401...|\n",
      "| 17|     100005|[Delta, Vero, 1-H...|[shower, only, fa...|     2.67|[Update, your, ba...|               [NaN]|      [-0.0255495711850...|        [0.00966486958965...|              [-0.0288202002993...|             [0.04687127470970...|-0.21867091136658928| 0.24864506918666573|-0.22062736959474996|[-0.2186709113665...|\n",
      "+---+-----------+--------------------+--------------------+---------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[-0.2656165585297...|\n",
      "|[-0.5286525620027...|\n",
      "|[0.06326357543589...|\n",
      "|[-0.3833220770401...|\n",
      "|[-0.2186709113665...|\n",
      "|[-0.4729628239332...|\n",
      "|[0.36700203744483...|\n",
      "|[-0.0859321242232...|\n",
      "|[-0.4298118503483...|\n",
      "|[0.02996571342180...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# call the idfUDF function to calculate the Cosine Similarity between each two vectors\n",
    "training = training.withColumn(\"searchAndTitle\", idfUDF(\"search_term_array_features\",\"product_title_array_features\"))\n",
    "training = training.withColumn(\"searchAndDescription\", idfUDF(\"search_term_array_features\",\"product_description_array_features\"))\n",
    "training = training.withColumn(\"searchAndAttributes\", idfUDF(\"search_term_array_features\",\"product_attributes_array_features\"))\n",
    "\n",
    "\n",
    "# combine three features into a dictionary\n",
    "features=[\"searchAndTitle\", \"searchAndDescription\", \"searchAndAttributes\"]\n",
    "assembler_features = VectorAssembler(inputCols=features, outputCol='features')\n",
    "training = assembler_features.transform(training)\n",
    "training.show(5)\n",
    "training.select(\"features\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|product_uid|       product_title|         search_term| product_description|  product_attributes|\n",
      "+---+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  1|     100001|[Simpson, Strong-...|[90, degree, brac...|[Not, only, do, a...|[Versatile, conne...|\n",
      "|  4|     100001|[Simpson, Strong-...|[metal, l, brackets]|[Not, only, do, a...|[Versatile, conne...|\n",
      "|  5|     100001|[Simpson, Strong-...|[simpson, sku, able]|[Not, only, do, a...|[Versatile, conne...|\n",
      "|  6|     100001|[Simpson, Strong-...|[simpson, strong,...|[Not, only, do, a...|[Versatile, conne...|\n",
      "|  7|     100001|[Simpson, Strong-...|[simpson, strong,...|[Not, only, do, a...|[Versatile, conne...|\n",
      "+---+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Processing the testing data, tokeninzing columns\n",
    "test_and_description = pd.merge(test_data, product_description, how=\"left\", on=\"product_uid\")\n",
    "test_description_attributes = pd.merge(test_and_description, product_attributes, how=\"left\", on=\"product_uid\")\n",
    "fields = [StructField(\"id\", StringType(), True), StructField(\"product_uid\", StringType(), True), StructField(\"product_title\", StringType(), True), StructField(\"search_term\", StringType(), True)\n",
    "         , StructField(\"product_description\", StringType(), True), StructField(\"product_attributes\", StringType(), True)]\n",
    "mergeSchema = StructType(fields)\n",
    "testing = sqlContext.createDataFrame(test_description_attributes, mergeSchema)\n",
    "split_col = functions.split(testing[3], \" \")\n",
    "testing = testing.withColumn('search_term', split_col)\n",
    "split_col = functions.split(testing[2], \" \")\n",
    "testing = testing.withColumn('product_title', split_col)\n",
    "split_col = functions.split(testing[4], \" \")\n",
    "testing = testing.withColumn('product_description', split_col)\n",
    "split_col = functions.split(testing[5], \" \")\n",
    "testing = testing.withColumn('product_attributes', split_col)\n",
    "testing.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+--------------------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+\n",
      "| id|product_uid|       product_title|         search_term| product_description|  product_attributes|search_term_array_features|product_title_array_features|product_description_array_features|product_attributes_array_features|\n",
      "+---+-----------+--------------------+--------------------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+\n",
      "|  1|     100001|[Simpson, Strong-...|[90, degree, brac...|[Not, only, do, a...|[Versatile, conne...|      [0.02333864212657...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...|\n",
      "|  4|     100001|[Simpson, Strong-...|[metal, l, brackets]|[Not, only, do, a...|[Versatile, conne...|      [0.01961279846727...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...|\n",
      "|  5|     100001|[Simpson, Strong-...|[simpson, sku, able]|[Not, only, do, a...|[Versatile, conne...|      [0.02016615595978...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...|\n",
      "|  6|     100001|[Simpson, Strong-...|[simpson, strong,...|[Not, only, do, a...|[Versatile, conne...|      [0.00186795881018...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...|\n",
      "|  7|     100001|[Simpson, Strong-...|[simpson, strong,...|[Not, only, do, a...|[Versatile, conne...|      [-0.0187256857752...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...|\n",
      "+---+-----------+--------------------+--------------------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting all tokens to vectors\n",
    "word2Vec = Word2Vec(vectorSize=10, minCount=0, inputCol=\"search_term\", outputCol=\"search_term_array_features\")\n",
    "model = word2Vec.fit(testing)\n",
    "testing = model.transform(testing)\n",
    "word2Vec = Word2Vec(vectorSize=10, minCount=0, inputCol=\"product_title\", outputCol=\"product_title_array_features\")\n",
    "model = word2Vec.fit(testing)\n",
    "testing = model.transform(testing)\n",
    "word2Vec = Word2Vec(vectorSize=10, minCount=0, inputCol=\"product_description\", outputCol=\"product_description_array_features\")\n",
    "model = word2Vec.fit(testing)\n",
    "testing = model.transform(testing)\n",
    "word2Vec = Word2Vec(vectorSize=10, minCount=0, inputCol=\"product_attributes\", outputCol=\"product_attributes_array_features\")\n",
    "model = word2Vec.fit(testing)\n",
    "testing = model.transform(testing)\n",
    "testing.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+--------------------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "| id|product_uid|       product_title|         search_term| product_description|  product_attributes|search_term_array_features|product_title_array_features|product_description_array_features|product_attributes_array_features|     searchAndTitle|searchAndDescription| searchAndAttributes|            features|\n",
      "+---+-----------+--------------------+--------------------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|  1|     100001|[Simpson, Strong-...|[90, degree, brac...|[Not, only, do, a...|[Versatile, conne...|      [0.02333864212657...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...|0.07666116684138458|   0.256937340940076|0.003059249579176...|[0.07666116684138...|\n",
      "|  4|     100001|[Simpson, Strong-...|[metal, l, brackets]|[Not, only, do, a...|[Versatile, conne...|      [0.01961279846727...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...|-0.2321998267041947| -0.7015241673920676|  0.6519098812936012|[-0.2321998267041...|\n",
      "|  5|     100001|[Simpson, Strong-...|[simpson, sku, able]|[Not, only, do, a...|[Versatile, conne...|      [0.02016615595978...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...| 0.1405525431314962|  0.5196730011103987|-0.13077171446035837|[0.14055254313149...|\n",
      "|  6|     100001|[Simpson, Strong-...|[simpson, strong,...|[Not, only, do, a...|[Versatile, conne...|      [0.00186795881018...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...| 0.4674581935497344|  0.3396417768050495|-0.11690136448342217|[0.46745819354973...|\n",
      "|  7|     100001|[Simpson, Strong-...|[simpson, strong,...|[Not, only, do, a...|[Versatile, conne...|      [-0.0187256857752...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...|0.13674995243563903|   0.369989251353545|-0.44053245189218043|[0.13674995243563...|\n",
      "+---+-----------+--------------------+--------------------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[0.07666116684138...|\n",
      "|[-0.2321998267041...|\n",
      "|[0.14055254313149...|\n",
      "|[0.46745819354973...|\n",
      "|[0.13674995243563...|\n",
      "|[-0.2683588501363...|\n",
      "|[0.75591810711742...|\n",
      "|[0.43020069942675...|\n",
      "|[0.16586526440348...|\n",
      "|[-0.2379296446658...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding Cosine Similarity between vectors\n",
    "testing = testing.withColumn(\"searchAndTitle\", idfUDF(\"search_term_array_features\",\"product_title_array_features\"))\n",
    "testing = testing.withColumn(\"searchAndDescription\", idfUDF(\"search_term_array_features\",\"product_description_array_features\"))\n",
    "testing = testing.withColumn(\"searchAndAttributes\", idfUDF(\"search_term_array_features\",\"product_attributes_array_features\"))\n",
    "features=[\"searchAndTitle\", \"searchAndDescription\", \"searchAndAttributes\"]\n",
    "assembler_features = VectorAssembler(inputCols=features, outputCol='features')\n",
    "testing = assembler_features.transform(testing)\n",
    "testing.show(5)\n",
    "testing.select(\"features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building pipeline using assembled 3 features and RandomForestRegressor as estimator, train it with training data, and use it on testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+--------------------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+-------------------+--------------------+--------------------+--------------------+------------------+\n",
      "| id|product_uid|       product_title|         search_term| product_description|  product_attributes|search_term_array_features|product_title_array_features|product_description_array_features|product_attributes_array_features|     searchAndTitle|searchAndDescription| searchAndAttributes|            features|        prediction|\n",
      "+---+-----------+--------------------+--------------------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+-------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|  1|     100001|[Simpson, Strong-...|[90, degree, brac...|[Not, only, do, a...|[Versatile, conne...|      [0.02333864212657...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...|0.07666116684138458|   0.256937340940076|0.003059249579176...|[0.07666116684138...| 2.316199810727489|\n",
      "|  4|     100001|[Simpson, Strong-...|[metal, l, brackets]|[Not, only, do, a...|[Versatile, conne...|      [0.01961279846727...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...|-0.2321998267041947| -0.7015241673920676|  0.6519098812936012|[-0.2321998267041...|2.8700816017316018|\n",
      "|  5|     100001|[Simpson, Strong-...|[simpson, sku, able]|[Not, only, do, a...|[Versatile, conne...|      [0.02016615595978...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...| 0.1405525431314962|  0.5196730011103987|-0.13077171446035837|[0.14055254313149...| 2.480921273586955|\n",
      "|  6|     100001|[Simpson, Strong-...|[simpson, strong,...|[Not, only, do, a...|[Versatile, conne...|      [0.00186795881018...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...| 0.4674581935497344|  0.3396417768050495|-0.11690136448342217|[0.46745819354973...| 2.565411425102106|\n",
      "|  7|     100001|[Simpson, Strong-...|[simpson, strong,...|[Not, only, do, a...|[Versatile, conne...|      [-0.0187256857752...|        [3.53743555024266...|              [0.03069015102219...|             [0.04154385153267...|0.13674995243563903|   0.369989251353545|-0.44053245189218043|[0.13674995243563...|2.4709336473243275|\n",
      "+---+-----------+--------------------+--------------------+--------------------+--------------------+--------------------------+----------------------------+----------------------------------+---------------------------------+-------------------+--------------------+--------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using thr Random Forest Regressor on the data\n",
    "rf = RandomForestRegressor(featuresCol=\"features\",labelCol='relevance', numTrees=20, maxDepth=5)\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "# train the model\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# testing\n",
    "prediction = model.transform(testing)\n",
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|        prediction|\n",
      "+---+------------------+\n",
      "|  1| 2.316199810727489|\n",
      "|  4|2.8700816017316018|\n",
      "|  5| 2.480921273586955|\n",
      "|  6| 2.565411425102106|\n",
      "|  7|2.4709336473243275|\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Writing prediction to a csv file on instance\n",
    "submission=prediction.select(\"id\",\"prediction\")\n",
    "submission.show(5)\n",
    "submission = submission.toPandas()\n",
    "\n",
    "# output the result into a csv file\n",
    "submission.to_csv('/home/jiawenz1_c4gcp/answer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Observation and Analysis: \n",
    "\n",
    "All predictions from this pipeline landed in the range of 2.30 - 2.46, this biased prediction might resulted form:\n",
    "\n",
    "(1)uneven length of text under each column, for example the product description is much longer than extracted attributes or product title, the data intensity in much lower in  the product description after it is distributed into 10 dimensions than that of attribues and product title;\n",
    "\n",
    "(2)cosine similarity can only convey the angles between objectives and features, but not the distance between features and objectives in the 10 dimension space;\n",
    "\n",
    "(3) We didn't choose the optimum number of dimension for each variables when using the Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4: Improve transformers\n",
    "\n",
    "Based on the observation in stage 2, we took 3 actions to improve our transformers:\n",
    "\n",
    "(1) We added Euclidean distance and used it with Word2Vec so as to take distance information into the model;\n",
    "\n",
    "(2) We added Word match function as transformer to obtain absolute values to get rid off the issue about the different length of product_description, product_title, and product_attribues, which should not be a matter in this problem.\n",
    "\n",
    "(2) We used Word2Vec + CosineSimilarity and Word2Vec + EuclideanDistance only on product_title and product_attributs that have similar length, and used WordMatch for all three variables with search_term.\n",
    "\n",
    "In the end we obtained 7 features and trained a new model with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to find euclidean distance between vectors\n",
    "def euclideanDistance(v1,v2):\n",
    "    dist = 0\n",
    "    if len(v1) < len(v2):\n",
    "        for i in range(len(v2)):\n",
    "            if i < len(v1):\n",
    "                dist += (v2[i] - v1[i]) * (v2[i] - v1[i])\n",
    "            else:\n",
    "                dist += v2[i] * v2[i]\n",
    "    else:\n",
    "        for i in range(len(v1)):\n",
    "            if i < len(v2):\n",
    "                dist += (v2[i] - v1[i]) * (v2[i] - v1[i])\n",
    "            else:\n",
    "                dist += v1[i] * v1[i]\n",
    "    return float(math.sqrt(dist))\n",
    "\n",
    "edUDF=udf(euclideanDistance, DoubleType())\n",
    "# v1 = [10, 10, 10]\n",
    "# v2 = [5, 5]\n",
    "# print (euclideanDistance(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to find how many words match between vectors\n",
    "def numberOfWordsMatched(v1,v2):\n",
    "    l1 = len(v1)\n",
    "    l2 = len(v2)\n",
    "    match = 0\n",
    "    for i in range(l1):\n",
    "        v1[i] = v1[i].lower()\n",
    "        for j in range(l2):\n",
    "            v2[j] = v2[j].lower()\n",
    "            if v1[i] == v2[j]:\n",
    "                match += 2\n",
    "            elif v1[i] in v2[j]:\n",
    "                match += 1\n",
    "            elif v2[j] in v1[i]:\n",
    "                match += 1\n",
    "            else:\n",
    "                match += 0\n",
    "    return match\n",
    "matchUDF=udf(numberOfWordsMatched, IntegerType())\n",
    "\n",
    "# v1 = [\"hi\", \"my\", \"name\"]\n",
    "# v2 = [\"hi\", \"abc\", \"names\"]\n",
    "# print (numberOfWordsMatched(v1,v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call the idfUDF function to calculate the Cosine Similarity between each two vectors\n",
    "training = training.withColumn(\"searchAndTitle\", idfUDF(\"search_term_array_features\",\"product_title_array_features\"))\n",
    "training = training.withColumn(\"searchAndAttributes\", idfUDF(\"search_term_array_features\",\"product_attributes_array_features\"))\n",
    "\n",
    "# call the edUDF function to calculate the Eucledian Distance between each two vectors\n",
    "training = training.withColumn(\"searchAndTitleED\", edUDF(\"search_term_array_features\",\"product_title_array_features\"))\n",
    "training = training.withColumn(\"searchAndAttributesED\", edUDF(\"search_term_array_features\",\"product_attributes_array_features\"))\n",
    "\n",
    "# call the matchUDF function to calculate the Eucledian Distance between each two vectors\n",
    "training = training.withColumn(\"searchAndTitleMD\", matchUDF(\"search_term_\",\"product_title_\"))\n",
    "training = training.withColumn(\"searchAndAttributesMD\", matchUDF(\"search_term_\",\"product_attributes_\"))\n",
    "training = training.withColumn(\"searchAndDescriptionMD\", matchUDF(\"search_term_\",\"product_description_\"))\n",
    "\n",
    "# combine three features into a dictionary\n",
    "features=[\"searchAndTitleED\", \"searchAndTitleMD\", \"searchAndAttributesED\", \"searchAndTitle\", \"searchAndAttributes\", \"searchAndAttributesMD\", \"searchAndDescriptionMD\"]\n",
    "assembler_features = VectorAssembler(inputCols=features, outputCol='features')\n",
    "training = assembler_features.transform(training)\n",
    "training.select(\"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "+--------------------+\n",
    "|            features|\n",
    "+--------------------+\n",
    "|[0.10091605379817...|\n",
    "|[0.07750586746402...|\n",
    "|[0.06845418730860...|\n",
    "|[0.06421587612470...|\n",
    "|[0.05030394370615...|\n",
    "|[0.10812486168510...|\n",
    "|[0.12025158840720...|\n",
    "|[0.12464254207317...|\n",
    "|[0.08713947684420...|\n",
    "|[0.23118286971610...|\n",
    "+--------------------+\n",
    "only showing top 10 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Processing testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding Cosine Similarity between vectors\n",
    "testing = testing.withColumn(\"searchAndTitle\", idfUDF(\"search_term_array_features\",\"product_title_array_features\"))\n",
    "testing = testing.withColumn(\"searchAndAttributes\", idfUDF(\"search_term_array_features\",\"product_attributes_array_features\"))\n",
    "\n",
    "# call the edUDF function to calculate the Eucledian Distance between each two vectors\n",
    "testing = testing.withColumn(\"searchAndTitleED\", edUDF(\"search_term_array_features\",\"product_title_array_features\"))\n",
    "testing = testing.withColumn(\"searchAndAttributesED\", edUDF(\"search_term_array_features\",\"product_attributes_array_features\"))\n",
    "\n",
    "# call the matchUDF function to calculate the Eucledian Distance between each two vectors\n",
    "testing = testing.withColumn(\"searchAndTitleMD\", matchUDF(\"search_term_\",\"product_title_\"))\n",
    "testing = testing.withColumn(\"searchAndAttributesMD\", matchUDF(\"search_term_\",\"product_attributes_\"))\n",
    "testing = testing.withColumn(\"searchAndDescriptionMD\", matchUDF(\"search_term_\",\"product_description_\"))\n",
    "\n",
    "# combine seven features into a dictionary\n",
    "features=[\"searchAndTitleED\", \"searchAndTitleMD\", \"searchAndAttributesED\", \"searchAndTitle\", \"searchAndAttributes\", \"searchAndAttributesMD\", \"searchAndDescriptionMD\"]\n",
    "\n",
    "assembler_features = VectorAssembler(inputCols=features, outputCol='features')\n",
    "testing = assembler_features.transform(testing)\n",
    "testing.show(5)\n",
    "testing.select(\"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "+---+-----------+--------------------+--------------------+---------------------+--------------------+--------------------+--------------------------+----------------------------+---------------------------------+-------------------+-------------------+-------------------+---------------------+----------------+---------------------+----------------------+--------------------+\n",
    "| id|product_uid|        search_term_|      product_title_|product_description__|product_description_| product_attributes_|search_term_array_features|product_title_array_features|product_attributes_array_features|     searchAndTitle|searchAndAttributes|   searchAndTitleED|searchAndAttributesED|searchAndTitleMD|searchAndAttributesMD|searchAndDescriptionMD|            features|\n",
    "+---+-----------+--------------------+--------------------+---------------------+--------------------+--------------------+--------------------------+----------------------------+---------------------------------+-------------------+-------------------+-------------------+---------------------+----------------+---------------------+----------------------+--------------------+\n",
    "|  1|     100001|[90, degree, brac...|[simpson, strong-...| [not, only, do, a...|[angles, make, jo...|[simpson, strong-...|      [-0.0024249718214...|        [0.00304099731147...|             [0.01872349058329...|  0.093711767633554| -0.522947684207269|0.08035624995514339|  0.13405254361682262|               0|                    0|                     2|[0.08035624995514...|\n",
    "|  4|     100001|[metal, l, brackets]|[simpson, strong-...| [not, only, do, a...|[angles, make, jo...|[simpson, strong-...|      [-3.4288999934991...|        [0.00304099731147...|             [0.01872349058329...|-0.3534827826772235|-0.4786740616270032|0.09198408567391715|   0.1263812523222371|               1|                    2|                    16|[0.09198408567391...|\n",
    "|  5|     100001|[simpson, sku, able]|[simpson, strong-...| [not, only, do, a...|[angles, make, jo...|[simpson, strong-...|      [0.03450785391032...|        [0.00304099731147...|             [0.01872349058329...| 0.4075075070864091| 0.5159938517326743|0.07981403728049065|  0.08806955741436671|               2|                    2|                     2|[0.07981403728049...|\n",
    "|  6|     100001|[simpson, strong,...|[simpson, strong-...| [not, only, do, a...|[angles, make, jo...|[simpson, strong-...|      [0.03005663189105...|        [0.00304099731147...|             [0.01872349058329...|0.08004009342072974| 0.5270192377343106|0.07978304307903628|  0.08175556069006426|               7|                   12|                    93|[0.07978304307903...|\n",
    "|  7|     100001|[simpson, strong,...|[simpson, strong-...| [not, only, do, a...|[angles, make, jo...|[simpson, strong-...|      [0.01446894637774...|        [0.00304099731147...|             [0.01872349058329...|-0.2801532572581965| 0.4502219749129761|0.09059617577360442|  0.08605488307188348|               4|                    4|                     8|[0.09059617577360...|\n",
    "+---+-----------+--------------------+--------------------+---------------------+--------------------+--------------------+--------------------------+----------------------------+---------------------------------+-------------------+-------------------+-------------------+---------------------+----------------+---------------------+----------------------+--------------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "+--------------------+\n",
    "|            features|\n",
    "+--------------------+\n",
    "|[0.08035624995514...|\n",
    "|[0.09198408567391...|\n",
    "|[0.07981403728049...|\n",
    "|[0.07978304307903...|\n",
    "|[0.09059617577360...|\n",
    "|[0.12605135382358...|\n",
    "|[0.24375789452819...|\n",
    "|[0.27385664587816...|\n",
    "|[0.25270126794471...|\n",
    "|[0.07805833249089...|\n",
    "+--------------------+\n",
    "only showing top 10 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building pipeline using assembled 7 features and RandomForestRegressor as estimator, train it with training data, and use it on testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using thr Random Forest Regressor on the data\n",
    "rf = RandomForestRegressor(featuresCol=\"features\",labelCol='relevance', numTrees=20, maxDepth=5)\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "# train the model\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# testing\n",
    "prediction = model.transform(testing)\n",
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing prediction to a csv file on instance\n",
    "submission=prediction.select(\"id\",\"prediction\")\n",
    "submission.show(100)\n",
    "submission = submission.toPandas()\n",
    "\n",
    "# output the result into a csv file\n",
    "submission.to_csv('/home/jiawenz1_c4gcp/answer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "+---+------------------+\n",
    "| id|        prediction|\n",
    "+---+------------------+\n",
    "|  1|2.2531105570770817|\n",
    "|  4| 2.516205260017714|\n",
    "|  5| 2.412751772787061|\n",
    "|  6|2.1677275793650796|\n",
    "|  7| 2.564859307760412|\n",
    "|  8| 2.212191500385579|\n",
    "| 10|2.6471629641729106|\n",
    "| 11| 2.492132776452756|\n",
    "| 12| 2.482303198701883|\n",
    "| 13|2.6193777209717926|\n",
    "| 14| 2.711583790979911|\n",
    "| 15|  2.57199076228244|\n",
    "| 19|1.9151565923207223|\n",
    "| 22|2.6101884256926033|\n",
    "| 24|2.4518054676506185|\n",
    "| 25| 2.436263331926154|\n",
    "| 26| 2.113331976576055|\n",
    "| 28|2.4895287863373605|\n",
    "| 29| 2.206066819693464|\n",
    "| 30|2.3932106557464983|\n",
    "| 31|2.5763032222239843|\n",
    "| 32|2.4656891664266207|\n",
    "| 33| 2.466463631936975|\n",
    "| 36| 2.424626153453097|\n",
    "| 39| 2.250550207324413|\n",
    "| 40|1.7955178571428572|\n",
    "| 41| 2.473871612864385|\n",
    "| 42|2.4394108173186435|\n",
    "| 43| 2.371889957996507|\n",
    "| 44|2.5031172417230647|\n",
    "| 45|2.7344864137339733|\n",
    "| 46|2.6756648451065232|\n",
    "| 47| 1.764080357142857|\n",
    "| 49|2.6200956479104724|\n",
    "| 50|2.5818523734218033|\n",
    "| 52| 2.556875901291691|\n",
    "| 53| 2.405162823234474|\n",
    "| 54| 2.159102978033318|\n",
    "| 55| 2.606694042533913|\n",
    "| 56| 2.436363143747295|\n",
    "| 57|2.4425536634053087|\n",
    "| 58|  2.48120410904154|\n",
    "| 59|2.4781460623997367|\n",
    "| 60|2.0468611119144016|\n",
    "| 61|2.5714128027112695|\n",
    "| 62|2.5315011553963203|\n",
    "| 63| 2.605281762527837|\n",
    "| 64|2.4644916643939885|\n",
    "| 66|2.5766399940083162|\n",
    "| 67|2.4260711637974204|\n",
    "| 68| 2.637975941692466|\n",
    "| 70|2.6068759416924663|\n",
    "| 71|2.4218581256005125|\n",
    "| 72|2.2476848370927316|\n",
    "| 73| 2.226784837092732|\n",
    "| 74|2.2476848370927316|\n",
    "| 76| 2.285351503759398|\n",
    "| 77|2.2476848370927316|\n",
    "| 78|2.3152618874835857|\n",
    "| 79| 2.655402412280702|\n",
    "| 80|  2.45280272556391|\n",
    "| 82|2.2786070593149543|\n",
    "| 83|2.4218581256005125|\n",
    "| 84| 2.410501254975675|\n",
    "| 86|2.4180532296821817|\n",
    "| 87|2.4639621896924844|\n",
    "| 89|1.5790178571428573|\n",
    "| 91| 2.241784433514889|\n",
    "| 93| 2.414751582391984|\n",
    "| 94|  2.42480272556391|\n",
    "| 95|  2.45280272556391|\n",
    "| 96|2.2662408521303257|\n",
    "| 97|2.2449196982841717|\n",
    "| 98|2.3914245293728724|\n",
    "| 99| 2.095840852130326|\n",
    "|100|2.4732351518926583|\n",
    "|102|2.4298898488131924|\n",
    "|103| 2.141519698284172|\n",
    "|104|2.2781756193368037|\n",
    "|107|2.2584320593149547|\n",
    "|108|2.3160756193368037|\n",
    "|109| 2.577900775258216|\n",
    "|110|2.4435420585824597|\n",
    "|111|2.4333337252491267|\n",
    "|112|2.5939898939890984|\n",
    "|115| 2.234850097162551|\n",
    "|116|2.6904339203559866|\n",
    "|118|2.3391990738894095|\n",
    "|119| 2.678346858232286|\n",
    "|121| 2.524744201229788|\n",
    "|124|2.6193961504589724|\n",
    "|126|2.5424690087689634|\n",
    "|128|2.4000426392578142|\n",
    "|129|2.5503720494153717|\n",
    "|130|2.3557056159118197|\n",
    "|131|  2.26387079557869|\n",
    "|132|2.3263606677033977|\n",
    "|133| 2.491199104760635|\n",
    "|134|2.5995409746962594|\n",
    "|135|2.6029163445094663|\n",
    "+---+------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 5: Expand on models\n",
    "\n",
    "After previous optimizations, we have achieved reasonably good result. We wanted to experiment on Linear regression to see if we can achieve better scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the Linear Regression on the data\n",
    "lr = LinearRegression(maxIter=100, regParam=0.3, elasticNetParam=0.8, labelCol='relevance')\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# train the model\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# testing\n",
    "prediction = model.transform(testing)\n",
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing prediction to a csv file on instance\n",
    "submission=prediction.select(\"id\",\"prediction\")\n",
    "submission.show(100)\n",
    "submission = submission.toPandas()\n",
    "\n",
    "# output the result into a csv file\n",
    "submission.to_csv('/home/jiawenz1_c4gcp/answer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "+---+------------------+\n",
    "| id|        prediction|\n",
    "+---+------------------+\n",
    "|  1|2.2531105570770817|\n",
    "|  4| 2.516205260017714|\n",
    "|  5| 2.412751772787061|\n",
    "|  6|2.1677275793650796|\n",
    "|  7| 2.564859307760412|\n",
    "|  8| 2.212191500385579|\n",
    "| 10|2.6471629641729106|\n",
    "| 11| 2.492132776452756|\n",
    "| 12| 2.482303198701883|\n",
    "| 13|2.6193777209717926|\n",
    "| 14| 2.711583790979911|\n",
    "| 15|  2.57199076228244|\n",
    "| 19|1.9151565923207223|\n",
    "| 22|2.6101884256926033|\n",
    "| 24|2.4518054676506185|\n",
    "| 25| 2.436263331926154|\n",
    "| 26| 2.113331976576055|\n",
    "| 28|2.4895287863373605|\n",
    "| 29| 2.206066819693464|\n",
    "| 30|2.3932106557464983|\n",
    "| 31|2.5763032222239843|\n",
    "| 32|2.4656891664266207|\n",
    "| 33| 2.466463631936975|\n",
    "| 36| 2.424626153453097|\n",
    "| 39| 2.250550207324413|\n",
    "| 40|1.7955178571428572|\n",
    "| 41| 2.473871612864385|\n",
    "| 42|2.4394108173186435|\n",
    "| 43| 2.371889957996507|\n",
    "| 44|2.5031172417230647|\n",
    "| 45|2.7344864137339733|\n",
    "| 46|2.6756648451065232|\n",
    "| 47| 1.764080357142857|\n",
    "| 49|2.6200956479104724|\n",
    "| 50|2.5818523734218033|\n",
    "| 52| 2.556875901291691|\n",
    "| 53| 2.405162823234474|\n",
    "| 54| 2.159102978033318|\n",
    "| 55| 2.606694042533913|\n",
    "| 56| 2.436363143747295|\n",
    "| 57|2.4425536634053087|\n",
    "| 58|  2.48120410904154|\n",
    "| 59|2.4781460623997367|\n",
    "| 60|2.0468611119144016|\n",
    "| 61|2.5714128027112695|\n",
    "| 62|2.5315011553963203|\n",
    "| 63| 2.605281762527837|\n",
    "| 64|2.4644916643939885|\n",
    "| 66|2.5766399940083162|\n",
    "| 67|2.4260711637974204|\n",
    "| 68| 2.637975941692466|\n",
    "| 70|2.6068759416924663|\n",
    "| 71|2.4218581256005125|\n",
    "| 72|2.2476848370927316|\n",
    "| 73| 2.226784837092732|\n",
    "| 74|2.2476848370927316|\n",
    "| 76| 2.285351503759398|\n",
    "| 77|2.2476848370927316|\n",
    "| 78|2.3152618874835857|\n",
    "| 79| 2.655402412280702|\n",
    "| 80|  2.45280272556391|\n",
    "| 82|2.2786070593149543|\n",
    "| 83|2.4218581256005125|\n",
    "| 84| 2.410501254975675|\n",
    "| 86|2.4180532296821817|\n",
    "| 87|2.4639621896924844|\n",
    "| 89|1.5790178571428573|\n",
    "| 91| 2.241784433514889|\n",
    "| 93| 2.414751582391984|\n",
    "| 94|  2.42480272556391|\n",
    "| 95|  2.45280272556391|\n",
    "| 96|2.2662408521303257|\n",
    "| 97|2.2449196982841717|\n",
    "| 98|2.3914245293728724|\n",
    "| 99| 2.095840852130326|\n",
    "|100|2.4732351518926583|\n",
    "|102|2.4298898488131924|\n",
    "|103| 2.141519698284172|\n",
    "|104|2.2781756193368037|\n",
    "|107|2.2584320593149547|\n",
    "|108|2.3160756193368037|\n",
    "|109| 2.577900775258216|\n",
    "|110|2.4435420585824597|\n",
    "|111|2.4333337252491267|\n",
    "|112|2.5939898939890984|\n",
    "|115| 2.234850097162551|\n",
    "|116|2.6904339203559866|\n",
    "|118|2.3391990738894095|\n",
    "|119| 2.678346858232286|\n",
    "|121| 2.524744201229788|\n",
    "|124|2.6193961504589724|\n",
    "|126|2.5424690087689634|\n",
    "|128|2.4000426392578142|\n",
    "|129|2.5503720494153717|\n",
    "|130|2.3557056159118197|\n",
    "|131|  2.26387079557869|\n",
    "|132|2.3263606677033977|\n",
    "|133| 2.491199104760635|\n",
    "|134|2.5995409746962594|\n",
    "|135|2.6029163445094663|\n",
    "+---+------------------+"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
